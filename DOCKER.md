# ML Framework Dockeréƒ¨ç½²æŒ‡å—

## ğŸ³ æ¦‚è¿°

ML Frameworkæä¾›äº†å®Œæ•´çš„DockeråŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œæ”¯æŒNVIDIA 4090 + CUDA 12.4çš„GPUåŠ é€Ÿç¯å¢ƒã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

### ç³»ç»Ÿè¦æ±‚
- Docker Desktop 20.10+ (Windows/Mac) æˆ– Docker Engine (Linux)
- NVIDIA GPUé©±åŠ¨ (å¦‚éœ€GPUæ”¯æŒ)
- NVIDIA Container Toolkit (å¦‚éœ€GPUæ”¯æŒ)

### GPUæ”¯æŒé…ç½®
1. **å®‰è£…NVIDIAé©±åŠ¨**ï¼šç¡®ä¿å®‰è£…æœ€æ–°çš„NVIDIAé©±åŠ¨
2. **å®‰è£…Docker Desktop**ï¼šä»å®˜ç½‘ä¸‹è½½å¹¶å®‰è£…
3. **é…ç½®NVIDIA Container Toolkit**ï¼š
   ```bash
   # Linux
   curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
   curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
     sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
     sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
   sudo apt-get update
   sudo apt-get install -y nvidia-container-toolkit
   sudo systemctl restart docker
   ```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1ï¼šä½¿ç”¨å¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

**Windows**ï¼š
```cmd
start_docker.bat
```

**Linux/Mac**ï¼š
```bash
chmod +x start_docker.sh
./start_docker.sh
```

### æ–¹æ³•2ï¼šä½¿ç”¨Pythonæ„å»ºè„šæœ¬

```bash
# æ„å»ºå®Œæ•´ç‰ˆé•œåƒ
python build_docker.py --type full

# æ„å»ºç”Ÿäº§ç‰ˆé•œåƒ
python build_docker.py --type prod

# æ„å»ºå¹¶éƒ¨ç½²æœåŠ¡
python build_docker.py --deploy

# æŸ¥çœ‹ä½¿ç”¨è¯´æ˜
python build_docker.py --usage
```

### æ–¹æ³•3ï¼šæ‰‹åŠ¨æ„å»ºå’Œè¿è¡Œ

```bash
# 1. æ„å»ºé•œåƒ
docker build -t ml-framework:latest .

# 2. è¿è¡Œå®¹å™¨ï¼ˆé€‰æ‹©ä¸€ç§ï¼‰
# Jupyter Notebook
docker run -it --gpus all -p 8888:8888 -v $(pwd)/data:/app/data ml-framework:latest jupyter

# FastAPIæœåŠ¡
docker run -it --gpus all -p 8000:8000 -v $(pwd)/data:/app/data ml-framework:latest api

# äº¤äº’å¼Shell
docker run -it --gpus all -v $(pwd)/data:/app/data ml-framework:latest shell
```

## ğŸ› ï¸ Docker Composeéƒ¨ç½²

### å¯åŠ¨æ‰€æœ‰æœåŠ¡
```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f ml-framework

# åœæ­¢æœåŠ¡
docker-compose down
```

### å¯ç”¨æœåŠ¡
- **Jupyter Notebook**: http://localhost:8888
- **FastAPI API**: http://localhost:8000
- **Streamlitåº”ç”¨**: http://localhost:8501
- **MLflow**: http://localhost:5000
- **PostgreSQL**: localhost:5432
- **Redis**: localhost:6379

## ğŸ”§ é•œåƒå˜ä½“

### å®Œæ•´ç‰ˆé•œåƒ (Dockerfile)
- åŒ…å«æ‰€æœ‰ä¾èµ–å’Œå·¥å…·
- æ”¯æŒJupyterã€å¼€å‘å·¥å…·
- é€‚åˆå¼€å‘å’Œå®éªŒ
- é•œåƒå¤§å°ï¼š~8GB

### ç”Ÿäº§ç‰ˆé•œåƒ (Dockerfile.prod)
- åªåŒ…å«è¿è¡Œæ—¶ä¾èµ–
- ä¼˜åŒ–çš„è½»é‡çº§é•œåƒ
- é€‚åˆç”Ÿäº§éƒ¨ç½²
- é•œåƒå¤§å°ï¼š~4GB

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### 1. è®­ç»ƒæ¨¡å‹
```bash
# å‡†å¤‡æ•°æ®æ–‡ä»¶åˆ°dataç›®å½•
docker run --gpus all \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  ml-framework:latest \
  train --data data/my_dataset.csv --target target_column
```

### 2. å¯åŠ¨APIæœåŠ¡
```bash
docker run -d --gpus all \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  --name ml-api \
  ml-framework:latest api
```

### 3. æ‰¹é‡é¢„æµ‹
```bash
docker run --gpus all \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  ml-framework:latest \
  predict --model models/my_model.joblib --data data/new_data.csv
```

### 4. å¼€å‘ç¯å¢ƒ
```bash
# å¯åŠ¨å¸¦æœ‰å·æŒ‚è½½çš„å¼€å‘ç¯å¢ƒ
docker run -it --gpus all \
  -p 8888:8888 \
  -v $(pwd):/app \
  ml-framework:latest shell

# åœ¨å®¹å™¨å†…
python examples/basic_classification.py
jupyter notebook --ip=0.0.0.0 --allow-root
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°æ®ç®¡ç†
```bash
# åˆ›å»ºæ•°æ®å·
docker volume create ml-data
docker volume create ml-models

# ä½¿ç”¨æ•°æ®å·
docker run --gpus all \
  -v ml-data:/app/data \
  -v ml-models:/app/models \
  ml-framework:latest
```

### 2. ç¯å¢ƒå˜é‡é…ç½®
```bash
# ä½¿ç”¨ç¯å¢ƒæ–‡ä»¶
echo "CUDA_VISIBLE_DEVICES=0" > .env
echo "BATCH_SIZE=128" >> .env

docker run --gpus all --env-file .env ml-framework:latest
```

### 3. ç½‘ç»œé…ç½®
```bash
# åˆ›å»ºè‡ªå®šä¹‰ç½‘ç»œ
docker network create ml-network

# åœ¨ç½‘ç»œä¸­è¿è¡ŒæœåŠ¡
docker run --gpus all --network ml-network ml-framework:latest
```

### 4. èµ„æºé™åˆ¶
```bash
# é™åˆ¶GPUå†…å­˜å’ŒCPUä½¿ç”¨
docker run --gpus all \
  --memory=16g \
  --cpus=8 \
  --shm-size=8g \
  ml-framework:latest
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. GPUä¸å¯ç”¨**
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥Docker GPUæ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.4-base-ubuntu22.04 nvidia-smi

# æ£€æŸ¥å®¹å™¨å†…GPU
docker run --rm --gpus all ml-framework:latest python -c "import torch; print(torch.cuda.is_available())"
```

**2. å†…å­˜ä¸è¶³**
```bash
# å¢åŠ Dockerå†…å­˜é™åˆ¶
docker run --gpus all --memory=32g --shm-size=16g ml-framework:latest

# æ¸…ç†Dockerç¼“å­˜
docker system prune -f
docker volume prune -f
```

**3. ç«¯å£å†²çª**
```bash
# ä½¿ç”¨ä¸åŒç«¯å£
docker run --gpus all -p 8889:8888 ml-framework:latest jupyter

# æŸ¥çœ‹ç«¯å£ä½¿ç”¨
netstat -tulpn | grep :8888
```

**4. æƒé™é—®é¢˜**
```bash
# ä½¿ç”¨ç”¨æˆ·IDæ˜ å°„
docker run --gpus all -u $(id -u):$(id -g) ml-framework:latest

# ä¿®æ”¹æ–‡ä»¶æƒé™
sudo chown -R $USER:$USER data/ models/
```

### æ—¥å¿—è°ƒè¯•
```bash
# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs container_name

# å®æ—¶æŸ¥çœ‹æ—¥å¿—
docker logs -f container_name

# è¿›å…¥è¿è¡Œä¸­çš„å®¹å™¨
docker exec -it container_name /bin/bash
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. GPUæ€§èƒ½
```bash
# å¯ç”¨æ‰€æœ‰GPU
docker run --gpus all ml-framework:latest

# æŒ‡å®šç‰¹å®šGPU
docker run --gpus device=0 ml-framework:latest

# è®¾ç½®GPUå†…å­˜é™åˆ¶
docker run --gpus all --env CUDA_MEM_FRACTION=0.8 ml-framework:latest
```

### 2. ç½‘ç»œæ€§èƒ½
```bash
# ä½¿ç”¨hostç½‘ç»œæ¨¡å¼
docker run --gpus all --network host ml-framework:latest

# ä¼˜åŒ–å…±äº«å†…å­˜
docker run --gpus all --shm-size=16g ml-framework:latest
```

### 3. å­˜å‚¨æ€§èƒ½
```bash
# ä½¿ç”¨tmpfs forä¸´æ—¶æ•°æ®
docker run --gpus all --tmpfs /tmp:rw,noexec,nosuid,size=4g ml-framework:latest

# ä½¿ç”¨ç»‘å®šæŒ‚è½½è€Œä¸æ˜¯å·
docker run --gpus all -v /host/path:/container/path:cached ml-framework:latest
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²

### Kuberneteséƒ¨ç½²
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-framework
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-framework
  template:
    metadata:
      labels:
        app: ml-framework
    spec:
      containers:
      - name: ml-framework
        image: ml-framework:latest
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
          requests:
            memory: "8Gi"
        ports:
        - containerPort: 8000
```

### Docker Swarméƒ¨ç½²
```bash
# åˆå§‹åŒ–Swarm
docker swarm init

# éƒ¨ç½²Stack
docker stack deploy -c docker-compose.yml ml-framework
```

è¿™ä¸ªDockeråŒ–æ–¹æ¡ˆä¸ºä½ çš„ML Frameworkæä¾›äº†å®Œæ•´çš„å®¹å™¨åŒ–æ”¯æŒï¼Œå……åˆ†åˆ©ç”¨äº†NVIDIA 4090çš„GPUæ€§èƒ½ï¼ğŸš€