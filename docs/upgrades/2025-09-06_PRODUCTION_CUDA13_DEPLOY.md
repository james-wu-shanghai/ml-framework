# ç”Ÿäº§ç¯å¢ƒ CUDA 13.0 éƒ¨ç½²æŒ‡å— - 2025å¹´9æœˆ6æ—¥

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿°å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²æ”¯æŒCUDA 13.0çš„ML Frameworkã€‚ç”Ÿäº§ç¯å¢ƒDockerfileå·²ä¼˜åŒ–ä¸ºè½»é‡çº§é•œåƒï¼Œä¸“æ³¨äºè¿è¡Œæ—¶æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚

## ğŸ—ï¸ ç”Ÿäº§ç¯å¢ƒç‰¹æ€§

### 1. é•œåƒä¼˜åŒ–
- **åŸºç¡€é•œåƒ**: `nvidia/cuda:13.0-runtime-ubuntu22.04` (ç²¾ç®€è¿è¡Œæ—¶ç‰ˆæœ¬)
- **é•œåƒå¤§å°**: ç›¸æ¯”å¼€å‘ç‰ˆæœ¬å‡å°‘çº¦40%
- **å®‰å…¨æ€§**: ç§»é™¤å¼€å‘å·¥å…·ï¼Œä»…ä¿ç•™è¿è¡Œæ—¶å¿…éœ€ç»„ä»¶
- **å¯åŠ¨é€Ÿåº¦**: ä¼˜åŒ–çš„ä¾èµ–å®‰è£…å’Œç¼“å­˜ç­–ç•¥

### 2. ä¾èµ–ç²¾ç®€
- **æ ¸å¿ƒMLåº“**: PyTorch 2.8+, TensorFlow 2.18+, Scikit-learn 1.5+
- **Webæ¡†æ¶**: FastAPI 0.115+, Flask 3.0+, Uvicorn 0.32+
- **æ•°æ®å¤„ç†**: NumPy 1.26+, Pandas 2.2+
- **ç§»é™¤ç»„ä»¶**: Jupyter, å¼€å‘å·¥å…·, å¯è§†åŒ–åº“

### 3. æ€§èƒ½ä¼˜åŒ–
- **Pythonå­—èŠ‚ç **: ç¦ç”¨`.pyc`æ–‡ä»¶å†™å…¥
- **ç¼“å­˜æ¸…ç†**: è‡ªåŠ¨æ¸…ç†pipç¼“å­˜
- **érootç”¨æˆ·**: å®‰å…¨çš„ç”¨æˆ·æƒé™è®¾ç½®

## ğŸš€ æ„å»ºå’Œéƒ¨ç½²

### 1. æ„å»ºç”Ÿäº§é•œåƒ

```bash
# åŸºç¡€æ„å»º
docker build -f Dockerfile.prod -t ml-framework:prod-cuda13 .

# å¸¦ç‰ˆæœ¬æ ‡ç­¾æ„å»º
docker build -f Dockerfile.prod -t ml-framework:v0.1.0-prod-cuda13 .

# å¤šå¹³å°æ„å»ºï¼ˆå¦‚éœ€è¦ï¼‰
docker buildx build --platform linux/amd64 -f Dockerfile.prod -t ml-framework:prod-cuda13 .
```

### 2. éªŒè¯é•œåƒ

```bash
# å¿«é€ŸéªŒè¯
docker run --gpus all ml-framework:prod-cuda13 python -c "import torch, tensorflow as tf; print('GPU:', torch.cuda.is_available())"

# å¥åº·æ£€æŸ¥
docker run --gpus all --rm ml-framework:prod-cuda13 python -c "
import torch
import tensorflow as tf
print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')
print(f'TensorFlow: {tf.__version__}, GPU: {len(tf.config.list_physical_devices(\"GPU\"))}')
"
```

### 3. ç”Ÿäº§ç¯å¢ƒè¿è¡Œ

#### APIæœåŠ¡éƒ¨ç½²
```bash
# å•å®¹å™¨éƒ¨ç½²
docker run -d \
  --name ml-framework-api \
  --gpus all \
  -p 8000:8000 \
  --restart unless-stopped \
  -e PYTHONUNBUFFERED=1 \
  -v /path/to/models:/app/models:ro \
  -v /path/to/logs:/app/logs \
  ml-framework:prod-cuda13

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health
```

#### ä½¿ç”¨Docker Compose
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  ml-api:
    image: ml-framework:prod-cuda13
    container_name: ml-framework-prod
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs
      - ./data:/app/data:ro
    environment:
      - PYTHONUNBUFFERED=1
      - ML_FRAMEWORK_LOGGING_LEVEL=INFO
      - ML_FRAMEWORK_MODELS_DIR=/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # å¯é€‰ï¼šRedisç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: ml-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # å¯é€‰ï¼šç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    container_name: ml-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml

volumes:
  redis_data:
```

#### éƒ¨ç½²å‘½ä»¤
```bash
# ä½¿ç”¨Docker Composeéƒ¨ç½²
docker-compose -f docker-compose.prod.yml up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.prod.yml logs -f ml-api

# æ‰©å®¹ï¼ˆå¦‚éœ€è¦ï¼‰
docker-compose -f docker-compose.prod.yml up -d --scale ml-api=3
```

## ğŸ”§ ç”Ÿäº§ç¯å¢ƒé…ç½®

### 1. ç¯å¢ƒå˜é‡é…ç½®

```bash
# æ ¸å¿ƒé…ç½®
export ML_FRAMEWORK_LOGGING_LEVEL=INFO
export ML_FRAMEWORK_MODELS_DIR=/app/models
export ML_FRAMEWORK_DATA_DIR=/app/data

# GPUé…ç½®
export CUDA_VISIBLE_DEVICES=0
export NVIDIA_VISIBLE_DEVICES=all

# æ€§èƒ½è°ƒä¼˜
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### 2. å·æŒ‚è½½é…ç½®

```bash
# æ¨¡å‹ç›®å½•ï¼ˆåªè¯»ï¼‰
-v /production/models:/app/models:ro

# æ•°æ®ç›®å½•ï¼ˆåªè¯»ï¼‰
-v /production/data:/app/data:ro

# æ—¥å¿—ç›®å½•ï¼ˆè¯»å†™ï¼‰
-v /production/logs:/app/logs

# é…ç½®æ–‡ä»¶
-v /production/config.yaml:/app/configs/production.yaml:ro
```

### 3. èµ„æºé™åˆ¶

```yaml
# Docker Composeèµ„æºé™åˆ¶
deploy:
  resources:
    limits:
      memory: 8G
      cpus: '4.0'
    reservations:
      memory: 4G
      cpus: '2.0'
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

## ğŸ” å®‰å…¨é…ç½®

### 1. ç”¨æˆ·æƒé™
- å®¹å™¨ä»¥érootç”¨æˆ·(mluser)è¿è¡Œ
- æ¨¡å‹å’Œæ•°æ®ç›®å½•ä½¿ç”¨åªè¯»æŒ‚è½½
- é™åˆ¶ç½‘ç»œè®¿é—®å’Œç«¯å£æš´éœ²

### 2. é•œåƒå®‰å…¨æ‰«æ

```bash
# ä½¿ç”¨Trivyæ‰«æ
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  aquasec/trivy:latest image ml-framework:prod-cuda13

# ä½¿ç”¨Clairæ‰«æï¼ˆå¦‚æœ‰ï¼‰
clair-scanner ml-framework:prod-cuda13
```

### 3. è¿è¡Œæ—¶å®‰å…¨

```bash
# é™åˆ¶å®¹å™¨æƒé™
docker run \
  --security-opt no-new-privileges \
  --read-only \
  --tmpfs /tmp \
  --gpus all \
  ml-framework:prod-cuda13
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### 1. å¥åº·æ£€æŸ¥

```bash
# APIå¥åº·æ£€æŸ¥ç«¯ç‚¹
GET /health
GET /metrics
GET /gpu-status
```

### 2. æ—¥å¿—é…ç½®

```python
# ç”Ÿäº§ç¯å¢ƒæ—¥å¿—é…ç½®
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    - type: "file"
      filename: "/app/logs/ml_framework.log"
      max_bytes: 10485760  # 10MB
      backup_count: 5
    - type: "console"
      stream: "stdout"
```

### 3. æ€§èƒ½ç›‘æ§

```bash
# GPUä½¿ç”¨ç›‘æ§
nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv --loop=1

# å®¹å™¨èµ„æºç›‘æ§
docker stats ml-framework-api

# åº”ç”¨ç›‘æ§ï¼ˆPrometheusæ ¼å¼ï¼‰
curl http://localhost:8000/metrics
```

## ğŸš¨ æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜

#### GPUä¸å¯ç”¨
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# æ£€æŸ¥å®¹å™¨GPUè®¿é—®
docker run --gpus all nvidia/cuda:13.0-base nvidia-smi

# éªŒè¯PyTorch GPUæ”¯æŒ
docker exec ml-framework-api python -c "import torch; print(torch.cuda.is_available())"
```

#### å†…å­˜ä¸è¶³
```bash
# æ£€æŸ¥GPUå†…å­˜
nvidia-smi

# æ£€æŸ¥ç³»ç»Ÿå†…å­˜
docker exec ml-framework-api free -h

# ä¼˜åŒ–å†…å­˜ä½¿ç”¨
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
```

#### æ€§èƒ½é—®é¢˜
```bash
# æ£€æŸ¥CPUä½¿ç”¨
docker exec ml-framework-api top

# æ£€æŸ¥IOæ€§èƒ½
docker exec ml-framework-api iostat -x 1

# ç½‘ç»œå»¶è¿Ÿæµ‹è¯•
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8000/health
```

### 2. æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs ml-framework-api --tail 100 -f

# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
docker exec ml-framework-api tail -f /app/logs/ml_framework.log

# æ—¥å¿—èšåˆï¼ˆå¦‚ä½¿ç”¨ELKï¼‰
# é…ç½®Filebeatæ”¶é›†å®¹å™¨æ—¥å¿—
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. CUDAä¼˜åŒ–

```bash
# ç¯å¢ƒå˜é‡ä¼˜åŒ–
export CUDA_LAUNCH_BLOCKING=0
export CUDA_CACHE_DISABLE=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# é¢„åˆ†é…GPUå†…å­˜
export TF_FORCE_GPU_ALLOW_GROWTH=true
```

### 2. å¹¶å‘ä¼˜åŒ–

```bash
# Uvicornå¹¶å‘é…ç½®
uvicorn src.ml_framework.api:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker
```

### 3. ç¼“å­˜ä¼˜åŒ–

```bash
# Redisç¼“å­˜é…ç½®
export ML_FRAMEWORK_CACHE_BACKEND=redis
export ML_FRAMEWORK_REDIS_URL=redis://redis:6379/0
```

## ğŸ”„ CI/CDé›†æˆ

### 1. æ„å»ºæµæ°´çº¿

```yaml
# .github/workflows/production-build.yml
name: Production Build
on:
  push:
    tags: ['v*']

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Build production image
        run: |
          docker build -f Dockerfile.prod \
            -t ${{ secrets.REGISTRY }}/ml-framework:${{ github.ref_name }}-prod-cuda13 .
          
      - name: Push to registry
        run: |
          docker push ${{ secrets.REGISTRY }}/ml-framework:${{ github.ref_name }}-prod-cuda13
```

### 2. éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh

set -e

TAG=${1:-latest}
REGISTRY=${REGISTRY:-your-registry.com}

echo "éƒ¨ç½²ML Frameworkç”Ÿäº§ç‰ˆæœ¬: $TAG"

# æ‹‰å–æœ€æ–°é•œåƒ
docker pull $REGISTRY/ml-framework:$TAG-prod-cuda13

# åœæ­¢ç°æœ‰æœåŠ¡
docker-compose -f docker-compose.prod.yml down

# æ›´æ–°é•œåƒæ ‡ç­¾
sed -i "s|image: ml-framework:.*|image: $REGISTRY/ml-framework:$TAG-prod-cuda13|" docker-compose.prod.yml

# å¯åŠ¨æœåŠ¡
docker-compose -f docker-compose.prod.yml up -d

# å¥åº·æ£€æŸ¥
sleep 30
curl -f http://localhost:8000/health || exit 1

echo "éƒ¨ç½²å®Œæˆï¼"
```

ç°åœ¨ä½ çš„ç”Ÿäº§ç¯å¢ƒå·²ç»å®Œå…¨æ”¯æŒCUDA 13.0äº†ï¼ğŸš€