"""
GPUåŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•NVIDIA 4090 + CUDA 12.4ç¯å¢ƒä¸‹çš„GPUåŠ é€ŸåŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import time
import numpy as np


def test_gpu_detection():
    """æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ” GPUæ£€æµ‹æµ‹è¯•")
    print("="*50)
    
    try:
        from ml_framework import gpu_manager
        
        # è·å–GPUä¿¡æ¯
        gpu_info = gpu_manager.get_gpu_info()
        
        print("GPUæ£€æµ‹ç»“æœ:")
        print(f"  PyTorchå¯ç”¨: {gpu_info['pytorch_available']}")
        print(f"  TensorFlowå¯ç”¨: {gpu_info['tensorflow_available']}")
        print(f"  CUDAå¯ç”¨: {gpu_info['cuda_available']}")
        
        if gpu_info['gpu_devices']:
            print("\nGPUè®¾å¤‡ä¿¡æ¯:")
            for device in gpu_info['gpu_devices']:
                print(f"  GPU {device['id']}: {device['name']}")
                print(f"    æ˜¾å­˜: {device['total_memory'] / 1024**3:.1f} GB")
                print(f"    è®¡ç®—èƒ½åŠ›: {device['major']}.{device['minor']}")
        
        # æ¨èæ‰¹æ¬¡å¤§å°
        batch_size = gpu_manager.recommend_batch_size('medium')
        print(f"\næ¨èæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUæ£€æµ‹å¤±è´¥: {e}")
        return False


def test_pytorch_gpu():
    """æµ‹è¯•PyTorch GPUåŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ”¥ PyTorch GPUæµ‹è¯•")
    print("="*50)
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âŒ PyTorch CUDAä¸å¯ç”¨")
            return False
        
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        print("\næµ‹è¯•GPUå¼ é‡æ“ä½œ...")
        device = torch.device('cuda:0')
        
        # CPU vs GPUæ€§èƒ½æµ‹è¯•
        size = 5000
        
        # CPUæµ‹è¯•
        start_time = time.time()
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        c_cpu = torch.mm(a_cpu, b_cpu)
        cpu_time = time.time() - start_time
        
        # GPUæµ‹è¯•
        start_time = time.time()
        a_gpu = torch.randn(size, size, device=device)
        b_gpu = torch.randn(size, size, device=device)
        torch.cuda.synchronize()  # ç¡®ä¿GPUæ“ä½œå®Œæˆ
        
        start_time = time.time()
        c_gpu = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"CPUçŸ©é˜µä¹˜æ³•æ—¶é—´: {cpu_time:.3f}ç§’")
        print(f"GPUçŸ©é˜µä¹˜æ³•æ—¶é—´: {gpu_time:.3f}ç§’")
        print(f"GPUåŠ é€Ÿæ¯”: {cpu_time/gpu_time:.1f}x")
        
        # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
        memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
        memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
        print(f"\nGPUå†…å­˜ä½¿ç”¨:")
        print(f"  å·²åˆ†é…: {memory_allocated:.1f} MB")
        print(f"  å·²ä¿ç•™: {memory_reserved:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ PyTorch GPUæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_tensorflow_gpu():
    """æµ‹è¯•TensorFlow GPUåŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ¤– TensorFlow GPUæµ‹è¯•")
    print("="*50)
    
    try:
        import tensorflow as tf
        
        print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
        
        # æ£€æŸ¥GPUè®¾å¤‡
        gpus = tf.config.list_physical_devices('GPU')
        if not gpus:
            print("âŒ TensorFlow GPUä¸å¯ç”¨")
            return False
        
        print(f"å¯ç”¨GPUæ•°é‡: {len(gpus)}")
        for i, gpu in enumerate(gpus):
            print(f"  GPU {i}: {gpu}")
        
        # GPU vs CPUæ€§èƒ½æµ‹è¯•
        print("\næµ‹è¯•GPUå¼ é‡æ“ä½œ...")
        size = 5000
        
        # CPUæµ‹è¯•
        with tf.device('/CPU:0'):
            start_time = time.time()
            a_cpu = tf.random.normal([size, size])
            b_cpu = tf.random.normal([size, size])
            c_cpu = tf.linalg.matmul(a_cpu, b_cpu)
            cpu_time = time.time() - start_time
        
        # GPUæµ‹è¯•
        with tf.device('/GPU:0'):
            start_time = time.time()
            a_gpu = tf.random.normal([size, size])
            b_gpu = tf.random.normal([size, size])
            c_gpu = tf.linalg.matmul(a_gpu, b_gpu)
            gpu_time = time.time() - start_time
        
        print(f"CPUçŸ©é˜µä¹˜æ³•æ—¶é—´: {cpu_time:.3f}ç§’")
        print(f"GPUçŸ©é˜µä¹˜æ³•æ—¶é—´: {gpu_time:.3f}ç§’")
        print(f"GPUåŠ é€Ÿæ¯”: {cpu_time/gpu_time:.1f}x")
        
        # æµ‹è¯•æ··åˆç²¾åº¦
        try:
            policy = tf.keras.mixed_precision.global_policy()
            print(f"\næ··åˆç²¾åº¦ç­–ç•¥: {policy.name}")
        except Exception as e:
            print(f"æ··åˆç²¾åº¦æ£€æŸ¥å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ TensorFlow GPUæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_ml_framework_gpu():
    """æµ‹è¯•MLæ¡†æ¶çš„GPUåŠŸèƒ½"""
    print("\n" + "="*50)
    print("ğŸ§  ML Framework GPUæµ‹è¯•")
    print("="*50)
    
    try:
        from ml_framework import MLFramework, gpu_manager
        import pandas as pd
        from sklearn.datasets import make_classification
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        X, y = make_classification(
            n_samples=2000,
            n_features=50,
            n_informative=30,
            n_classes=2,
            random_state=42
        )
        
        feature_names = [f'feature_{i}' for i in range(X.shape[1])]
        data = pd.DataFrame(X, columns=feature_names)
        data['target'] = y
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        os.makedirs('data', exist_ok=True)
        data.to_csv('data/gpu_test_data.csv', index=False)
        
        # ä½¿ç”¨GPUé…ç½®åˆå§‹åŒ–æ¡†æ¶
        framework = MLFramework(config_path='configs/gpu_config.yaml')
        
        # ä¼˜åŒ–GPUè®¾ç½®
        gpu_manager.optimize_pytorch_gpu()
        gpu_manager.optimize_tensorflow_gpu()
        
        # æ¨èGPUæ‰¹æ¬¡å¤§å°
        recommended_batch_size = gpu_manager.recommend_batch_size('medium')
        framework.config.set('data.batch_size', recommended_batch_size)
        
        print(f"ä½¿ç”¨GPUä¼˜åŒ–æ‰¹æ¬¡å¤§å°: {recommended_batch_size}")
        
        # æµ‹è¯•å®Œæ•´çš„MLæµç¨‹
        framework.load_data('data/gpu_test_data.csv', target_column='target')
        framework.set_task_type('classification')
        framework.preprocess_data()
        
        # ä½¿ç”¨æ›´å¤§çš„éšæœºæ£®æ—ï¼ˆåˆ©ç”¨CPUå¤šæ ¸ï¼‰
        framework.select_model('random_forest', n_estimators=200, n_jobs=-1)
        
        start_time = time.time()
        framework.train()
        training_time = time.time() - start_time
        
        results = framework.evaluate()
        
        print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"å‡†ç¡®ç‡: {results.get('accuracy', 0):.4f}")
        
        # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨
        memory_info = gpu_manager.get_memory_usage()
        if memory_info:
            print("\nGPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
            for framework_name, devices in memory_info.items():
                for device, info in devices.items():
                    allocated_mb = info['allocated'] / 1024**2
                    print(f"  {framework_name} {device}: {allocated_mb:.1f} MB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ML Framework GPUæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ NVIDIA 4090 + CUDA 12.4 GPUåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("GPUæ£€æµ‹", test_gpu_detection()))
    results.append(("PyTorch GPU", test_pytorch_gpu()))
    results.append(("TensorFlow GPU", test_tensorflow_gpu()))
    results.append(("ML Framework GPU", test_ml_framework_gpu()))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*60)
    print("ğŸ† GPUæµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*60)
    
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:20} {status}")
        if result:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("\nğŸ‰ æ‰€æœ‰GPUåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("ä½ çš„NVIDIA 4090 + CUDA 12.4ç¯å¢ƒé…ç½®å®Œç¾ï¼")
        
        print("\nğŸ’¡ GPUä½¿ç”¨å»ºè®®:")
        print("1. ä½¿ç”¨å¤§æ‰¹æ¬¡è®­ç»ƒä»¥å……åˆ†åˆ©ç”¨GPUæ€§èƒ½")
        print("2. å®šæœŸæ¸…ç†GPUç¼“å­˜: torch.cuda.empty_cache()")
        print("3. ç›‘æ§GPUæ¸©åº¦å’Œå†…å­˜ä½¿ç”¨")
        print("4. è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒä»¥æå‡æ€§èƒ½")
        
        return 0
    else:
        print("\nâš ï¸ éƒ¨åˆ†GPUåŠŸèƒ½å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
        print("1. NVIDIAé©±åŠ¨æ˜¯å¦æœ€æ–°")
        print("2. CUDA 12.4æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("3. PyTorch/TensorFlowæ˜¯å¦æ”¯æŒCUDA 12.4")
        
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)