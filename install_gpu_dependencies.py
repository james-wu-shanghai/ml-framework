"""
GPUä¼˜åŒ–ä¾èµ–å®‰è£…è„šæœ¬

ä¸“é—¨ä¸ºNVIDIA 4090 + CUDA 12.4ç¯å¢ƒä¼˜åŒ–çš„å®‰è£…è„šæœ¬
"""

import subprocess
import sys
import os


def run_command(command, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº¦"""
    print(f"\nğŸ”§ {description}...")
    print(f"æ‰§è¡Œå‘½ä»¤: {command}")
    
    try:
        result = subprocess.run(command, shell=True, check=True, 
                              capture_output=True, text=True)
        print(f"âœ… {description} æˆåŠŸ")
        if result.stdout:
            print(f"è¾“å‡º: {result.stdout.strip()}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} å¤±è´¥")
        print(f"é”™è¯¯: {e.stderr}")
        return False


def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥GPUç¯å¢ƒ...")
    
    # æ£€æŸ¥NVIDIA-SMI
    try:
        result = subprocess.run("nvidia-smi", shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… NVIDIAé©±åŠ¨æ£€æµ‹æˆåŠŸ")
            print("GPUä¿¡æ¯:")
            print(result.stdout.split('\n')[8:12])  # æ˜¾ç¤ºGPUä¿¡æ¯è¡Œ
        else:
            print("âŒ æœªæ£€æµ‹åˆ°NVIDIAé©±åŠ¨")
            return False
    except FileNotFoundError:
        print("âŒ nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å®‰è£…äº†NVIDIAé©±åŠ¨")
        return False
    
    # æ£€æŸ¥CUDAç‰ˆæœ¬
    try:
        result = subprocess.run("nvcc --version", shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… CUDAå·¥å…·åŒ…æ£€æµ‹æˆåŠŸ")
            cuda_info = [line for line in result.stdout.split('\n') if 'release' in line.lower()]
            if cuda_info:
                print(f"CUDAç‰ˆæœ¬: {cuda_info[0].strip()}")
        else:
            print("âš ï¸ CUDAå·¥å…·åŒ…æœªæ£€æµ‹åˆ°ï¼ˆè¿™å¯¹äºPyTorch/TensorFlowå¯èƒ½ä¸æ˜¯å¿…éœ€çš„ï¼‰")
    except FileNotFoundError:
        print("âš ï¸ nvccå‘½ä»¤æœªæ‰¾åˆ°ï¼ŒCUDAå·¥å…·åŒ…å¯èƒ½æœªå®‰è£…")
    
    return True


def install_pytorch_cuda():
    """å®‰è£…æ”¯æŒCUDA 12.4çš„PyTorch"""
    print("\nğŸ“¦ å®‰è£…PyTorch (CUDA 12.4æ”¯æŒ)...")
    
    # PyTorchå®˜æ–¹CUDA 12.4æ”¯æŒçš„å®‰è£…å‘½ä»¤
    pytorch_command = (
        "pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 "
        "--index-url https://download.pytorch.org/whl/cu124"
    )
    
    return run_command(pytorch_command, "PyTorch CUDAç‰ˆæœ¬å®‰è£…")


def install_tensorflow():
    """å®‰è£…TensorFlow"""
    print("\nğŸ“¦ å®‰è£…TensorFlow...")
    
    # TensorFlow 2.15+ åŸç”Ÿæ”¯æŒCUDA 12.4
    tensorflow_command = "pip install tensorflow==2.15.0"
    
    return run_command(tensorflow_command, "TensorFlowå®‰è£…")


def install_other_dependencies():
    """å®‰è£…å…¶ä»–ä¾èµ–"""
    print("\nğŸ“¦ å®‰è£…å…¶ä»–ä¾èµ–åŒ…...")
    
    # æ ¸å¿ƒä¾èµ–
    core_deps = [
        "numpy>=1.21.0",
        "pandas>=1.3.0", 
        "scikit-learn>=1.0.0",
        "matplotlib>=3.4.0",
        "seaborn>=0.11.0",
        "pyyaml>=5.4.0",
        "click>=8.0.0",
        "joblib>=1.0.0",
        "tqdm>=4.61.0"
    ]
    
    for dep in core_deps:
        if not run_command(f"pip install {dep}", f"å®‰è£… {dep.split('>=')[0]}"):
            return False
    
    return True


def verify_installation():
    """éªŒè¯å®‰è£…"""
    print("\nğŸ”¬ éªŒè¯GPUæ”¯æŒå®‰è£…...")
    
    # éªŒè¯PyTorch CUDAæ”¯æŒ
    pytorch_check = """
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")
"""
    
    try:
        result = subprocess.run([sys.executable, "-c", pytorch_check], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… PyTorchéªŒè¯:")
            print(result.stdout)
        else:
            print("âŒ PyTorchéªŒè¯å¤±è´¥:")
            print(result.stderr)
    except Exception as e:
        print(f"âŒ PyTorchéªŒè¯å‡ºé”™: {e}")
    
    # éªŒè¯TensorFlow GPUæ”¯æŒ
    tensorflow_check = """
import tensorflow as tf
print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
print(f"GPUå¯ç”¨: {len(tf.config.list_physical_devices('GPU')) > 0}")
if len(tf.config.list_physical_devices('GPU')) > 0:
    print(f"GPUè®¾å¤‡: {tf.config.list_physical_devices('GPU')}")
"""
    
    try:
        result = subprocess.run([sys.executable, "-c", tensorflow_check], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… TensorFlowéªŒè¯:")
            print(result.stdout)
        else:
            print("âŒ TensorFlowéªŒè¯å¤±è´¥:")
            print(result.stderr)
    except Exception as e:
        print(f"âŒ TensorFlowéªŒè¯å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ ML Framework GPUä¼˜åŒ–å®‰è£…è„šæœ¬")
    print("ğŸ¯ ç›®æ ‡ç¡¬ä»¶: NVIDIA 4090 + CUDA 12.4")
    print("=" * 60)
    
    # 1. æ£€æŸ¥GPUç¯å¢ƒ
    if not check_gpu_environment():
        print("\nâŒ GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥NVIDIAé©±åŠ¨å®‰è£…")
        return 1
    
    # 2. å®‰è£…PyTorch CUDAç‰ˆæœ¬
    if not install_pytorch_cuda():
        print("\nâŒ PyTorchå®‰è£…å¤±è´¥")
        return 1
    
    # 3. å®‰è£…TensorFlow
    if not install_tensorflow():
        print("\nâŒ TensorFlowå®‰è£…å¤±è´¥")
        return 1
    
    # 4. å®‰è£…å…¶ä»–ä¾èµ–
    if not install_other_dependencies():
        print("\nâŒ å…¶ä»–ä¾èµ–å®‰è£…å¤±è´¥")
        return 1
    
    # 5. éªŒè¯å®‰è£…
    verify_installation()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ GPUä¼˜åŒ–å®‰è£…å®Œæˆï¼")
    print("\nğŸ“ æ¥ä¸‹æ¥çš„æ­¥éª¤:")
    print("1. è¿è¡Œ 'python test_framework.py' æµ‹è¯•æ¡†æ¶")
    print("2. è¿è¡Œ 'python examples/basic_classification.py' æµ‹è¯•ç¤ºä¾‹")
    print("3. æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ: nvidia-smi")
    print("\nğŸ’¡ æç¤º:")
    print("- å¦‚æœé‡åˆ°CUDAå†…å­˜é—®é¢˜ï¼Œå¯ä»¥åœ¨ä»£ç ä¸­æ·»åŠ :")
    print("  torch.cuda.empty_cache()")
    print("- TensorFlow GPUå†…å­˜å¢é•¿æ§åˆ¶:")
    print("  tf.config.experimental.set_memory_growth(gpu, True)")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())