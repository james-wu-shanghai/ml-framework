# ML Framework Dockeré•œåƒ
# æ”¯æŒNVIDIA 4090 + CUDA 12.4çš„GPUåŠ é€Ÿæœºå™¨å­¦ä¹ æ¡†æ¶

# ä½¿ç”¨NVIDIAå®˜æ–¹CUDAåŸºç¡€é•œåƒï¼Œæ”¯æŒCUDA 12.4
FROM nvidia/cuda:12.4-devel-ubuntu22.04

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    python3.11-venv \
    git \
    wget \
    curl \
    build-essential \
    cmake \
    pkg-config \
    libhdf5-dev \
    libopenblas-dev \
    liblapack-dev \
    libjpeg-dev \
    libpng-dev \
    libtiff-dev \
    libavcodec-dev \
    libavformat-dev \
    libswscale-dev \
    libv4l-dev \
    libxvidcore-dev \
    libx264-dev \
    libgtk-3-dev \
    libatlas-base-dev \
    gfortran \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºPythonç¬¦å·é“¾æ¥
RUN ln -s /usr/bin/python3.11 /usr/bin/python && \
    ln -s /usr/bin/pip3 /usr/bin/pip

# å‡çº§pipå’Œå®‰è£…åŸºç¡€å·¥å…·
RUN pip install --upgrade pip setuptools wheel

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY requirements.txt .
COPY setup.py .
COPY MANIFEST.in .
COPY README.md .
COPY LICENSE .

# å¤åˆ¶æºä»£ç å’Œé…ç½®
COPY src/ src/
COPY configs/ configs/
COPY examples/ examples/
COPY docs/ docs/

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p data models logs plots tests

# å®‰è£…Pythonä¾èµ– - åˆ†é˜¶æ®µå®‰è£…ä»¥ä¼˜åŒ–ç¼“å­˜
# 1. å®‰è£…æ ¸å¿ƒä¾èµ–
RUN pip install --no-cache-dir \
    numpy>=1.21.0 \
    pandas>=1.3.0 \
    scikit-learn>=1.0.0 \
    scipy>=1.7.0 \
    matplotlib>=3.4.0 \
    seaborn>=0.11.0 \
    pyyaml>=5.4.0 \
    click>=8.0.0 \
    joblib>=1.0.0 \
    tqdm>=4.61.0

# 2. å®‰è£…PyTorch (CUDA 12.4æ”¯æŒ)
RUN pip install --no-cache-dir \
    torch==2.2.0 \
    torchvision==0.17.0 \
    torchaudio==2.2.0 \
    --index-url https://download.pytorch.org/whl/cu124

# 3. å®‰è£…TensorFlow (CUDA 12.4æ”¯æŒ)
RUN pip install --no-cache-dir \
    tensorflow==2.15.0 \
    keras>=3.0.0

# 4. å®‰è£…æ•°æ®å¤„ç†å’Œå¯è§†åŒ–åº“
RUN pip install --no-cache-dir \
    plotly>=5.0.0 \
    bokeh>=2.3.0 \
    openpyxl>=3.0.0 \
    xlrd>=2.0.0 \
    h5py>=3.1.0 \
    pillow>=8.0.0

# 5. å®‰è£…MLå·¥å…·åº“
RUN pip install --no-cache-dir \
    shap>=0.39.0 \
    lime>=0.2.0 \
    optuna>=2.8.0 \
    mlflow>=1.18.0 \
    wandb>=0.12.0 \
    tensorboard>=2.6.0

# 6. å®‰è£…Webæ¡†æ¶å’Œéƒ¨ç½²å·¥å…·
RUN pip install --no-cache-dir \
    flask>=2.0.0 \
    fastapi>=0.68.0 \
    uvicorn>=0.15.0 \
    streamlit>=0.84.0

# 7. å®‰è£…å¼€å‘å·¥å…·
RUN pip install --no-cache-dir \
    pytest>=6.2.0 \
    pytest-cov>=2.12.0 \
    black>=21.6.0 \
    jupyter>=1.0.0 \
    ipykernel>=6.0.0

# å®‰è£…é¡¹ç›®æœ¬èº«
RUN pip install -e .

# è®¾ç½®GPUç¯å¢ƒä¼˜åŒ–
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 mluser && \
    chown -R mluser:mluser /app
USER mluser

# æš´éœ²ç«¯å£
EXPOSE 8000 8080 8501 5000

# è®¾ç½®å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD python -c "import torch, tensorflow as tf; print('GPU:', torch.cuda.is_available(), len(tf.config.list_physical_devices('GPU')))" || exit 1

# åˆ›å»ºå¯åŠ¨è„šæœ¬
COPY <<EOF /app/start.sh
#!/bin/bash
set -e

echo "ğŸš€ ML Framework Dockerå®¹å™¨å¯åŠ¨"
echo "=================================="

# æ£€æŸ¥GPUç¯å¢ƒ
echo "ğŸ” æ£€æŸ¥GPUç¯å¢ƒ..."
python -c "
import torch
import tensorflow as tf
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'PyTorch CUDAå¯ç”¨: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}')
    print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')

print(f'TensorFlowç‰ˆæœ¬: {tf.__version__}')
tf_gpus = tf.config.list_physical_devices('GPU')
print(f'TensorFlow GPUå¯ç”¨: {len(tf_gpus) > 0}')
"

# è¿è¡ŒGPUä¼˜åŒ–
echo "âš™ï¸ åº”ç”¨GPUä¼˜åŒ–è®¾ç½®..."
python -c "
from src.ml_framework.gpu_utils import gpu_manager
gpu_manager.optimize_pytorch_gpu()
gpu_manager.optimize_tensorflow_gpu()
print('GPUä¼˜åŒ–å®Œæˆ')
"

# æ ¹æ®å‚æ•°å¯åŠ¨ä¸åŒæœåŠ¡
case "\$1" in
    "jupyter")
        echo "ğŸ““ å¯åŠ¨Jupyter Notebook..."
        jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=''
        ;;
    "api")
        echo "ğŸŒ å¯åŠ¨FastAPIæœåŠ¡..."
        uvicorn src.ml_framework.api:app --host 0.0.0.0 --port 8000
        ;;
    "streamlit")
        echo "ğŸ“Š å¯åŠ¨Streamlitåº”ç”¨..."
        streamlit run src/ml_framework/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
        ;;
    "test")
        echo "ğŸ§ª è¿è¡Œæµ‹è¯•..."
        python test_framework.py
        python test_gpu.py
        ;;
    "train")
        echo "ğŸ¯ å¯åŠ¨è®­ç»ƒæ¨¡å¼..."
        python -m ml_framework.cli train "\$@"
        ;;
    "shell")
        echo "ğŸ’» å¯åŠ¨äº¤äº’å¼shell..."
        /bin/bash
        ;;
    *)
        echo "ğŸ“– ML Frameworkä½¿ç”¨è¯´æ˜"
        echo "å¯ç”¨å‘½ä»¤:"
        echo "  jupyter   - å¯åŠ¨Jupyter Notebook (ç«¯å£8888)"
        echo "  api       - å¯åŠ¨FastAPIæœåŠ¡ (ç«¯å£8000)"
        echo "  streamlit - å¯åŠ¨Streamlitåº”ç”¨ (ç«¯å£8501)"
        echo "  test      - è¿è¡Œæ¡†æ¶æµ‹è¯•"
        echo "  train     - è®­ç»ƒæ¨¡å‹"
        echo "  shell     - å¯åŠ¨äº¤äº’å¼shell"
        echo ""
        echo "ç¤ºä¾‹:"
        echo "  docker run --gpus all -p 8888:8888 ml-framework jupyter"
        echo "  docker run --gpus all -p 8000:8000 ml-framework api"
        echo "  docker run --gpus all -v \$(pwd)/data:/app/data ml-framework train --data data/my_data.csv --target target"
        ;;
esac
EOF

RUN chmod +x /app/start.sh

# é»˜è®¤å¯åŠ¨å‘½ä»¤
CMD ["/app/start.sh"]