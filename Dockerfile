# ML Framework Dockeré•œåƒ
# æ”¯æŒNVIDIA GPU + CUDA 13.0çš„GPUåŠ é€Ÿæœºå™¨å­¦ä¹ æ¡†æ¶

# ä½¿ç”¨NVIDIAå®˜æ–¹CUDAåŸºç¡€é•œåƒï¼Œæ”¯æŒCUDA 13.0
FROM nvidia/cuda:13.0-devel-ubuntu22.04

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    python3.11-venv \
    git \
    wget \
    curl \
    build-essential \
    cmake \
    pkg-config \
    libhdf5-dev \
    libopenblas-dev \
    liblapack-dev \
    libjpeg-dev \
    libpng-dev \
    libtiff-dev \
    libavcodec-dev \
    libavformat-dev \
    libswscale-dev \
    libv4l-dev \
    libxvidcore-dev \
    libx264-dev \
    libgtk-3-dev \
    libatlas-base-dev \
    gfortran \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºPythonç¬¦å·é“¾æ¥
RUN ln -s /usr/bin/python3.11 /usr/bin/python && \
    ln -s /usr/bin/pip3 /usr/bin/pip

# å‡çº§pipå’Œå®‰è£…åŸºç¡€å·¥å…·
RUN pip install --upgrade pip setuptools wheel

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY requirements.txt .
COPY setup.py .
COPY MANIFEST.in .
COPY README.md .
COPY LICENSE .

# å¤åˆ¶æºä»£ç å’Œé…ç½®
COPY src/ src/
COPY configs/ configs/
COPY examples/ examples/
COPY docs/ docs/

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p data models logs plots tests

# å®‰è£…Pythonä¾èµ– - åˆ†é˜¶æ®µå®‰è£…ä»¥ä¼˜åŒ–ç¼“å­˜
# 1. å®‰è£…æ ¸å¿ƒä¾èµ–
RUN pip install --no-cache-dir \
    numpy>=1.26.0 \
    pandas>=2.2.0 \
    scikit-learn>=1.5.0 \
    scipy>=1.14.0 \
    matplotlib>=3.9.0 \
    seaborn>=0.13.0 \
    pyyaml>=5.4.0 \
    click>=8.0.0 \
    joblib>=1.0.0 \
    tqdm>=4.61.0

# 2. å®‰è£…PyTorch (CUDA 13.0æ”¯æŒ)
RUN pip install --no-cache-dir \
    torch>=2.8.0 \
    torchvision>=0.18.0 \
    torchaudio>=2.8.0 \
    --index-url https://download.pytorch.org/whl/cu130

# 3. å®‰è£…TensorFlow (CUDA 13.0æ”¯æŒ)
RUN pip install --no-cache-dir \
    tensorflow>=2.18.0 \
    keras>=3.6.0

# 4. å®‰è£…æ•°æ®å¤„ç†å’Œå¯è§†åŒ–åº“
RUN pip install --no-cache-dir \
    plotly>=5.24.0 \
    bokeh>=3.6.0 \
    openpyxl>=3.0.0 \
    xlrd>=2.0.0 \
    h5py>=3.1.0 \
    pillow>=8.0.0

# 5. å®‰è£…MLå·¥å…·åº“
RUN pip install --no-cache-dir \
    shap>=0.39.0 \
    lime>=0.2.0 \
    optuna>=4.0.0 \
    mlflow>=2.16.0 \
    wandb>=0.18.0 \
    tensorboard>=2.18.0

# 6. å®‰è£…Webæ¡†æ¶å’Œéƒ¨ç½²å·¥å…·
RUN pip install --no-cache-dir \
    flask>=3.0.0 \
    fastapi>=0.115.0 \
    uvicorn>=0.32.0 \
    streamlit>=1.40.0

# 7. å®‰è£…å¼€å‘å·¥å…·
RUN pip install --no-cache-dir \
    pytest>=8.3.0 \
    pytest-cov>=6.0.0 \
    black>=24.10.0 \
    jupyter>=1.1.0 \
    ipykernel>=6.29.0 \
    ipywidgets>=8.1.0

# å®‰è£…é¡¹ç›®æœ¬èº«
RUN pip install -e .

# è®¾ç½®GPUç¯å¢ƒä¼˜åŒ–
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
ENV CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 mluser && \
    chown -R mluser:mluser /app
USER mluser

# æš´éœ²ç«¯å£
EXPOSE 8000 8080 8501 5000

# è®¾ç½®å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD python -c "import torch, tensorflow as tf; print('GPU:', torch.cuda.is_available(), len(tf.config.list_physical_devices('GPU')))" || exit 1

# åˆ›å»ºå¯åŠ¨è„šæœ¬
COPY <<EOF /app/start.sh
#!/bin/bash
set -e

echo "ğŸš€ ML Framework Dockerå®¹å™¨å¯åŠ¨ (CUDA 13.0)"
echo "============================================"

# æ£€æŸ¥GPUç¯å¢ƒ
echo "ğŸ” æ£€æŸ¥GPUç¯å¢ƒ..."
python -c "
import torch
import tensorflow as tf
print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')
print(f'PyTorch CUDAå¯ç”¨: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}')
    print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')

print(f'TensorFlowç‰ˆæœ¬: {tf.__version__}')
tf_gpus = tf.config.list_physical_devices('GPU')
print(f'TensorFlow GPUå¯ç”¨: {len(tf_gpus) > 0}')
"

# è¿è¡ŒGPUä¼˜åŒ–
echo "âš™ï¸ åº”ç”¨GPUä¼˜åŒ–è®¾ç½®..."
python -c "
from src.ml_framework.gpu_utils import gpu_manager
gpu_manager.optimize_pytorch_gpu()
gpu_manager.optimize_tensorflow_gpu()
print('GPUä¼˜åŒ–å®Œæˆ')
"

# æ ¹æ®å‚æ•°å¯åŠ¨ä¸åŒæœåŠ¡
case "\$1" in
    "jupyter")
        echo "ğŸ““ å¯åŠ¨Jupyter Notebook..."
        jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=''
        ;;
    "api")
        echo "ğŸŒ å¯åŠ¨FastAPIæœåŠ¡ (CUDA 13.0æ”¯æŒ)..."
        uvicorn src.ml_framework.api:app --host 0.0.0.0 --port 8000
        ;;
    "streamlit")
        echo "ğŸ“Š å¯åŠ¨Streamlitåº”ç”¨ (CUDA 13.0æ”¯æŒ)..."
        streamlit run src/ml_framework/streamlit_app.py --server.port 8501 --server.address 0.0.0.0
        ;;
    "test")
        echo "ğŸ§ª è¿è¡Œæµ‹è¯• (CUDA 13.0ç¯å¢ƒ)..."
        python test_framework.py
        python test_gpu.py
        ;;
    "train")
        echo "ğŸ¯ å¯åŠ¨è®­ç»ƒæ¨¡å¼..."
        python -m ml_framework.cli train "\$@"
        ;;
    "shell")
        echo "ğŸ’» å¯åŠ¨äº¤äº’å¼shell (CUDA 13.0ç¯å¢ƒ)..."
        /bin/bash
        ;;
    *)
        echo "ğŸ“– ML Frameworkä½¿ç”¨è¯´æ˜"
        echo "å¯ç”¨å‘½ä»¤:"
        echo "  jupyter   - å¯åŠ¨Jupyter Notebook (ç«¯å£8888)"
        echo "  api       - å¯åŠ¨FastAPIæœåŠ¡ (ç«¯å£8000)"
        echo "  streamlit - å¯åŠ¨Streamlitåº”ç”¨ (ç«¯å£8501)"
        echo "  test      - è¿è¡Œæ¡†æ¶æµ‹è¯•"
        echo "  train     - è®­ç»ƒæ¨¡å‹"
        echo "  shell     - å¯åŠ¨äº¤äº’å¼shell"
        echo ""
        echo "ç¤ºä¾‹:"
        echo "  docker run --gpus all -p 8888:8888 ml-framework jupyter"
        echo "  docker run --gpus all -p 8000:8000 ml-framework api"
        echo "  docker run --gpus all -v \$(pwd)/data:/app/data ml-framework train --data data/my_data.csv --target target"
        ;;
esac
EOF

RUN chmod +x /app/start.sh

# é»˜è®¤å¯åŠ¨å‘½ä»¤
CMD ["/app/start.sh"]